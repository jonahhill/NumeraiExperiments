{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \"\"\"Alexandru most  dangerous ensemble \"\"\"\n",
    "import os\n",
    "__author__ = 'Daia Alexandru'\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "from sklearn.metrics import r2_score\n",
    "from __future__ import division\n",
    "import pandas as pd \n",
    "import  random as random\n",
    "import numpy as np\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import heapq\n",
    "from sklearn.linear_model import  LinearRegression,BayesianRidge\n",
    "from sklearn.ensemble import  RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import *\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import *\n",
    " \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import *\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    " \n",
    " \n",
    "class BadErrorTreshold(Exception):\n",
    "    pass\n",
    " \n",
    "class EnsembleRegressors(object):\n",
    "    ''''\n",
    "    ensembleRegressor is a class of ensembles generated by a  fix number of  iterations.This class is abstract \n",
    "    meaning  it  is not possible  to create instances  of it.,,stochasticLearning''  has interface  role. '''\n",
    "    def __init__(self,regressors,sampling,iterations,test,X,y,k_fold):\n",
    "        ''''intialize ensemble with  near all  parameters- see that   some of them are  intialized  by other methods'''\n",
    "        self.sampling=sampling#dropout(cv) ratio-percentage\n",
    "        if (self.sampling<=0.0 or self.sampling>=1.0):\n",
    "            raise ValueError('Sampling  must be percent like value between[0,1) ex 0.75, you introduced :',self.sampling)\n",
    " \n",
    " \n",
    "        self.test=test#final test set\n",
    "        self.X=X#original train features\n",
    "        self.y=y#original train outcome\n",
    "        self.iterations=iterations#number of  iterations in the  stochastic ensemble\n",
    "        self.regressors=regressors#ensemble models  for  each iteration      \n",
    "        self.randomIndexes=[ [] for rounds in range(self.iterations)]#filled in with random dropout  by getStochasticDataSets()\n",
    "        self.error=[]#error for  each averaged iteration\n",
    " \n",
    " \n",
    "        self.stochasticPredictions=[ [] for  rounds in range(self.iterations)]#predictions on random drop outs\n",
    "        self.finalPredictions=[]\n",
    " \n",
    "        self.x_train=[]#his   next  4 rows are intialized  by getStochasticDataSets() according to random chunks\n",
    "        self.y_train=[]\n",
    "        self.x_test=[]\n",
    "        self.y_test=[]\n",
    " \n",
    "        self.new_x_ts=[]\n",
    "        self.new_x_tr=[]\n",
    "        self.new_test_final=[]\n",
    "        self.k_folds=k_fold\n",
    "        \n",
    "    def returnError(self):\n",
    "        '''returns : error for  each iteration-just like cross-validation'''\n",
    "        return self.error\n",
    "    \n",
    "    def returnRandomIndexes(self):\n",
    "        '''returns: random  indexes  for  each dropout'''\n",
    "        return self.randomIndexes\n",
    "    \n",
    "    def returnstochasticPredictions(self):\n",
    "        '''returns:predictions  on random dropous'''\n",
    "        return self.stochasticPredictions\n",
    "    \n",
    "    def getStochasticDataSets(self):\n",
    "        '''\n",
    "        For some  potentially  iteration creates a random  idx variable representing the index  for random  dropout.\n",
    "        Splits the train in  train/test(x_train,y_train,features and outcome for cross val train-x_test,y_test for cross_val test\n",
    "        .Updated this train-test  variables with corresponding  values.See that  the initial vales are empty in the __init__.\n",
    "        returns: idx(index of  dropout rows).\n",
    "        \n",
    "        '''\n",
    "        x_train, x_test, y_train, y_test = train_test_split(self.X, self.y, test_size=self.sampling)\n",
    "        self.x_train=x_train\n",
    "        self.x_test=x_test\n",
    "        self.y_train=y_train\n",
    "        self.y_test=y_test\n",
    "        #train_i, test_i = train_test_split(np.arange(self.X.shape[0]), train_size = self.sampling)\n",
    "        #self.x_train=self.X[train_i]\n",
    "        #self.y_train=self.y[train_i]\n",
    "        #self.x_test=self.X[test_i]\n",
    "        #self.y_test=self.y[test_i]\n",
    "        #idx=test_i\n",
    "        #return  idx\n",
    "    \n",
    "    def training(self,i):\n",
    "        ''''\n",
    "        For a  particular iteration ,,i''  calls getStochasticDataSets() method.In this was the randomIndexes feature is\n",
    "        fiiled with corresponding index dropouts at position ,,i''.Since train/test new data sets according to cross validation\n",
    "        are   filled  by getStochasticDataSets()  independet  of iteration number , this data sets will update for each new iter.\n",
    "        performmed in stochasticLearning().'''\n",
    "        self.getStochasticDataSets()\n",
    "        #blending- we have x_train,y_train,x_test,y_test   our stochastic  dropouts for diversity\n",
    "        #we are  going first  to make blend train,blend test from this dropoutsthen will see evaluation on the dropout\n",
    "        clfs=self.regressors\n",
    "        n_folds=self.k_folds\n",
    "        skf=list(StratifiedKFold(self.y_train,n_folds))\n",
    "        blend_tr=np.zeros((self.x_train.shape[0],len(clfs)))\n",
    "        blend_ts=np.zeros((self.x_test.shape[0],len(clfs)))#for loca al evaluation\n",
    "        blend_final_test=np.zeros((self.test.shape[0],len(clfs)))\n",
    "        from sklearn.metrics  import  log_loss\n",
    "        for  j , clf in  enumerate(clfs):\n",
    "            print(clf)\n",
    "            blend_ts_j=np.zeros((self.x_test.shape[0],len(skf)))\n",
    "            blend_final_j=np.zeros((self.test.shape[0],len(skf)))\n",
    "            for i ,(tr,ts) in enumerate(skf):\n",
    "                print(\"fold\",i)\n",
    "                x_tr=self.x_train[tr]\n",
    "                y_tr=self.y_train[tr]\n",
    "                x_ts=self.x_train[ts]\n",
    "                y_ts=self.y_train[ts]\n",
    "                clf.fit(x_tr,y_tr)\n",
    "                y_sub=clf.predict_proba(x_ts)[:,1]\n",
    "                blend_tr[ts,j]=y_sub\n",
    "                blend_ts_j[:,i]=clf.predict_proba(self.x_test)[:,1]\n",
    "                blend_final_j[:,i]=clf.predict_proba(self.test)[:,1]\n",
    "            blend_ts[:,j]=blend_ts_j.mean(1)\n",
    "            blend_final_test[:,j]=blend_final_j.mean(1)\n",
    "        #blend on evaluation\n",
    "        import  xgboost as  xgb\n",
    "        clf = xgb.XGBClassifier(n_estimators=1)\n",
    "        clf.fit(blend_tr, self.y_train)\n",
    "        \n",
    "        pred_eval=  clf.predict_proba(blend_ts)[:,1]\n",
    "        self.error.append(log_loss(self.y_test,pred_eval))\n",
    "        #blend  on final test set:\n",
    "        final_sub=clf.predict_proba(blend_final_test)[:,1]\n",
    "        self.finalPredictions.append(final_sub)\n",
    "        \n",
    "        \n",
    " \n",
    "        return self\n",
    "    \n",
    "    def getFinalPrediction(self,errorTreshold):\n",
    "       raise NotImplementedError###averages models  according to best top prediction  trehsold  with different implementantions\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    def stochasticLearning(self):\n",
    "         raise NotImplementedError#Since EnsemebleRegressors class is abstract this will act  as an interface\n",
    "                                  #and will be implemented   by  this  EnsembleRegressors sublclasses  for averaging and\n",
    "                                  #weighted average\n",
    "    \n",
    "        \n",
    "class AveragingModels(EnsembleRegressors):\n",
    "    '''Inherits everything from EnsembleRegressors  class and implements stochastiLearning method  averaging models \n",
    "    for each iteration'''\n",
    " \n",
    "    def stochasticLearning(self):\n",
    "        \n",
    "        \n",
    "        for iter in range(self.iterations):\n",
    "            print('Stochastic Iteration number ',iter)\n",
    "            self.training(iter)\n",
    " \n",
    "    def getFinalPrediction(self,errorTreshold):\n",
    "        if (errorTreshold<=self.iterations and errorTreshold>0):\n",
    "            \n",
    "            topErrorIndex=heapq.nsmallest(errorTreshold, range(len(self.error)), self.error.__getitem__)\n",
    "            finalAvg=0\n",
    "            for topError in topErrorIndex:\n",
    "                finalAvg=finalAvg+self.finalPredictions[topError]\n",
    "            finalAvg=(finalAvg)/float(len(topErrorIndex))\n",
    "            return finalAvg\n",
    "        else:\n",
    "            raise BadErrorTreshold\n",
    "       \n",
    "       \n",
    "    \n",
    "                                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0  0.185868  0.895807  0.596433  0.028381  0.053369  0.202876  0.912054   \n",
      "1  0.709165  0.967550  0.529121  0.611704  0.961349  0.786213  0.489511   \n",
      "2  0.526553  0.015789  0.940873  0.867239  0.084545  0.033899  0.412348   \n",
      "\n",
      "   feature8  feature9  feature10   ...    feature13  feature14  feature15  \\\n",
      "0  0.669988  0.143481   0.132973   ...     0.855909   0.842887   0.913296   \n",
      "1  0.607460  0.715500   0.476030   ...     0.517913   0.279985   0.938501   \n",
      "2  0.538480  0.122290   0.370455   ...     0.992078   0.878373   0.090039   \n",
      "\n",
      "   feature16  feature17  feature18  feature19  feature20  feature21  target  \n",
      "0   0.938493   0.393211   0.211985   0.337367   0.898316   0.245452       0  \n",
      "1   0.490024   0.630741   0.117521   0.917893   0.894724   0.897643       1  \n",
      "2   0.379432   0.271485   0.361007   0.379741   0.028594   0.351007       0  \n",
      "\n",
      "[3 rows x 22 columns]\n",
      "(136573, 23)\n",
      "(136573,)\n"
     ]
    }
   ],
   "source": [
    "import  pandas  as  pd\n",
    "import numpy  as np\n",
    "train=pd.read_csv('/home/machine_learning/Downloads/numerai_datasets/numerai_training_data.csv')\n",
    "print(train.head(3))\n",
    "train1=pd.read_csv('/home/machine_learning/Downloads/numerai_datasets/trainCat.csv')\n",
    "train['sums']=train1['sums']\n",
    "train['kinetic']=train1['kinetic']\n",
    "y=train['target']\n",
    "train=train.drop(['target'],axis=1)\n",
    "print(train.shape)\n",
    "print(y.shape)\n",
    "test=pd.read_csv('/home/machine_learning/Downloads/numerai_datasets/numerai_tournament_data.csv')\n",
    "test1=pd.read_csv('/home/machine_learning/Downloads/numerai_datasets/testCat.csv')\n",
    "test['sums']=test1['sums']\n",
    "test['kinetic']=test1['kinetic']\n",
    "\n",
    "id=test['t_id']\n",
    "test=test.drop(['t_id'],axis=1)\n",
    "sub=pd.read_csv('/home/machine_learning/Downloads/numerai_datasets/example_predictions.csv')\n",
    "trainvec=np.array(train)\n",
    "testvec=np.array(test)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71520595524594721"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y,avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.1564793204614645"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg=np.average(trainvec[:,1:13],axis=1)\n",
    "avg2=np.average(trainvec[:,6:21],axis=1)\n",
    "avg3=(avg+avg2)/1\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y,avg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sub=pd.read_csv('/home/machine_learning/Downloads/numerai_datasets/example_predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost  as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Iteration number  0\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=10, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=5, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=15, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "Stochastic Iteration number  1\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=10, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=5, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=15, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "Stochastic Iteration number  2\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=10, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=5, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=15, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "fold 0\n",
      "fold 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\" run stuff with  0.1 dropout , 3 iterations,predict on testvec, train on trainvec and  y ,do 10\n",
    "stratified  sampling\"\"\"\n",
    "import  xgboost  as xgb\n",
    "clfs=[ xgb.XGBClassifier(n_estimators=10),xgb.XGBClassifier(n_estimators=5)\n",
    "      ,xgb.XGBClassifier(n_estimators=15),RandomForestClassifier(n_estimators=5),\n",
    "      \n",
    " \n",
    "      RandomForestClassifier(n_estimators=15)\n",
    " \n",
    " \n",
    " \n",
    "     ]\n",
    "o=AveragingModels(clfs,0.8,3,trainvec,trainvec,y,2)\n",
    "o.stochasticLearning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.69298162027794008, 0.69294892777155759, 0.69298553920791373]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50028318,  0.5030815 ,  0.49539718, ...,  0.50267196,\n",
       "        0.49669173,  0.50028318], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=o.getFinalPrediction(3)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3234</td>\n",
       "      <td>0.492083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2161</td>\n",
       "      <td>0.492083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3927</td>\n",
       "      <td>0.498085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   t_id  probability\n",
       "0  3234     0.492083\n",
       "1  2161     0.492083\n",
       "2  3927     0.498085"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sub['probability']=pred\n",
    "#sub.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sub.to_csv('/home/machine_learning/Downloads/sub10.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gplearn.genetic import SymbolicTransformer\n",
    "function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log', 'abs', 'neg', 'inv', 'max', 'min']\n",
    "gp = SymbolicTransformer(generations=10, population_size=200,\n",
    "                         hall_of_fame=30, n_components=20,\n",
    "                       \n",
    "                         parsimony_coefficient=0.0005,\n",
    "                         max_samples=0.9, verbose=1,\n",
    "                         random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |    Population Average   |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    10.36   0.213601638207        3    0.59285929376   0.589549415289      2.71m\n",
      "   1     8.56   0.430897286581        6   0.607727984482    0.61046670946      2.41m\n",
      "   2     4.92   0.515956360952       16   0.628716658074   0.636893727495      2.09m\n",
      "   3     7.96   0.498925426986        8   0.641193633833   0.625300060281      1.81m\n",
      "   4     9.82   0.506271324568       12   0.692093847797   0.690452083308      1.52m\n",
      "   5    10.92   0.523466323809       12   0.692112504267     0.6901428501      1.23m\n"
     ]
    }
   ],
   "source": [
    "gp.fit(trainvec, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_tr=gp.transform(trainvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_ts=gp.transform(testvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_train=np.hstack((trainvec,new_tr))\n",
    "other_test=np.hstack((testvec,new_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136573, 26)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Iteration number  0\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=10, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=5, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=15, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "Stochastic Iteration number  1\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=10, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=5, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=15, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "Stochastic Iteration number  2\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=10, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=5, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=15, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\" run stuff with  0.1 dropout , 3 iterations,predict on testvec, train on trainvec and  y ,do 10\n",
    "stratified  sampling\"\"\"\n",
    "import  xgboost  as xgb\n",
    "clfs=[ xgb.XGBClassifier(n_estimators=10),xgb.XGBClassifier(n_estimators=5)\n",
    "      ,xgb.XGBClassifier(n_estimators=15),RandomForestClassifier(n_estimators=5),\n",
    "      \n",
    " \n",
    "      RandomForestClassifier(n_estimators=15)\n",
    " \n",
    " \n",
    " \n",
    "     ]\n",
    "o=AveragingModels(clfs,0.8,3,other_test,other_train,y,3)\n",
    "o.stochasticLearning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.69295196932578629, 0.69294682301605526, 0.69292343683235114]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred=o.getFinalPrediction(1)\n",
    "pred\n",
    "sub['probability']=pred\n",
    "sub.to_csv('/home/machine_learning/Downloads/sub11.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
